{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd gdrive/My Drive/TRLWS"
      ],
      "metadata": {
        "id": "1y1BQ2aioCSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq \"xformers<0.0.24\""
      ],
      "metadata": {
        "id": "dgob6wVDMEkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ],
      "metadata": {
        "id": "whF17zGhKtEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq --no-deps \"trl<0.9.0\" peft accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "5dlNGaPON1Lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPJPw50wx5h7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from datasets import Dataset, load_dataset, concatenate_datasets\n",
        "from transformers import TrainingArguments, TextStreamer\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGX9wG7Lhc-z"
      },
      "outputs": [],
      "source": [
        "# Load model\n",
        "max_seq_length = 2048\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=True,\n",
        "    dtype=None,\n",
        ")\n",
        "\n",
        "# Prepare model for PEFT\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n",
        "    use_rslora=True,\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state = 0\n",
        ")\n",
        "print(model.print_trainable_parameters())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\", \"system\": \"system\"}, # ShareGPT style\n",
        "    map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
        ")\n",
        "\n",
        "def apply_template(examples):\n",
        "    # Prepend the system message\n",
        "    system_message = [{\"from\": \"system\", \"value\": \"You are a highly knowledgeable medical assistant with expertise in various medical domains. Your primary role is to provide accurate, evidence-based medical information and advice. Your responses should be grounded in the latest clinical guidelines and research, ensuring reliability and precision. When answering questions, consider the context and complexity of the query, and aim to provide the most relevant and correct answers.\"}]\n",
        "    formatted_conversations = []\n",
        "    for example in examples[\"conversations\"]:\n",
        "        # Combine the system message with the user/assistant conversation\n",
        "        conversation = system_message + example\n",
        "        formatted_conversations.append(conversation)\n",
        "\n",
        "    #messages = examples[\"conversations\"]\n",
        "    #text = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=False) for message in messages]\n",
        "    text = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in formatted_conversations]\n",
        "    return {\"text\": text}\n",
        "\n"
      ],
      "metadata": {
        "id": "D1DyErSek8E5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# peparing MedQA dataset\n",
        "\n",
        "medqa_dataset = load_dataset(\"GBaker/MedQA-USMLE-4-options-hf\", split=\"train\")\n",
        "\n",
        "converted_conversations = []\n",
        "\n",
        "for example in medqa_dataset:\n",
        "    # Extract the question and answer options\n",
        "    question = example['sent1']\n",
        "    #options = [example['ending0'], example['ending1'], example['ending2'], example['ending3']]\n",
        "    options = [\n",
        "        f\"A) {example['ending0']}\",\n",
        "        f\"B) {example['ending1']}\",\n",
        "        f\"C) {example['ending2']}\",\n",
        "        f\"D) {example['ending3']}\"\n",
        "    ]\n",
        "    #correct_answer = options[example['label']]\n",
        "    #correct_answer = chr(65 + example['label'])  # Convert 0-3 to A-D\n",
        "\n",
        "    correct_answer = f\"Answer: {options[example['label']]}\"\n",
        "\n",
        "    options_str = \"\\n\".join(options)\n",
        "    input_text = f\"Question: {question}\\nOptions:\\n{options_str}\"\n",
        "\n",
        "    # Create a ChatML-like conversation for each data point\n",
        "    conversation = [\n",
        "        {\"from\": \"human\", \"value\": input_text},\n",
        "        {\"from\": \"gpt\", \"value\": correct_answer}\n",
        "    ]\n",
        "\n",
        "    # Add the conversation to the list\n",
        "    converted_conversations.append(conversation)\n",
        "\n",
        "formatted_medqa_dataset = Dataset.from_dict({\"conversations\": converted_conversations})\n",
        "\n",
        "formatted_medqa_dataset = formatted_medqa_dataset.map(apply_template, batched=True)\n"
      ],
      "metadata": {
        "id": "kVtKUC3z1LS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# peparing MedMCQA dataset\n",
        "\n",
        "med_mcqa_dataset = load_dataset(\"openlifescienceai/medmcqa\", split=\"train\")\n",
        "\n",
        "converted_conversations = []\n",
        "\n",
        "for example in med_mcqa_dataset:\n",
        "    # Extract the question and answer options\n",
        "    question = example['question']\n",
        "    #options = [example['ending0'], example['ending1'], example['ending2'], example['ending3']]\n",
        "    options = [\n",
        "        f\"A) {example['opa']}\",\n",
        "        f\"B) {example['opb']}\",\n",
        "        f\"C) {example['opc']}\",\n",
        "        f\"D) {example['opd']}\"\n",
        "    ]\n",
        "    #correct_answer = options[example['label']]\n",
        "    #correct_answer = chr(65 + example['label'])  # Convert 0-3 to A-D\n",
        "\n",
        "    correct_answer = f\"Answer: {options[example['cop']]}\"\n",
        "\n",
        "    options_str = \"\\n\".join(options)\n",
        "    input_text = f\"Question: {question}\\nOptions:\\n{options_str}\"\n",
        "\n",
        "    # Create a ChatML-like conversation for each data point\n",
        "    conversation = [\n",
        "        {\"from\": \"human\", \"value\": input_text},\n",
        "        {\"from\": \"gpt\", \"value\": correct_answer}\n",
        "    ]\n",
        "\n",
        "    # Add the conversation to the list\n",
        "    converted_conversations.append(conversation)\n",
        "\n",
        "formatted_med_mcqa_dataset = Dataset.from_dict({\"conversations\": converted_conversations})\n",
        "\n",
        "#print(formatted_med_mcqa_dataset[0])\n",
        "\n",
        "formatted_med_mcqa_dataset = formatted_med_mcqa_dataset.map(apply_template, batched=True)"
      ],
      "metadata": {
        "id": "0pjEO_7MHZfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# peparing PubMedQA dataset\n",
        "\n",
        "pub_medqa_dataset = load_dataset(\"qiaojin/PubMedQA\", \"pqa_artificial\", split=\"train\")\n",
        "\n",
        "converted_conversations = []\n",
        "\n",
        "for example in pub_medqa_dataset:\n",
        "    # Extract the question and answer options\n",
        "    context = example['context']\n",
        "    #print(type(context))\n",
        "    #context = context['contexts'][0]\n",
        "    context = \" \".join(context['contexts'])\n",
        "    question = example['question']\n",
        "    #correct_answer = example['final_decision']\n",
        "    options = [\n",
        "        f\"A) yes\",\n",
        "        f\"B) no\",\n",
        "        f\"C) maybe\"\n",
        "    ]\n",
        "\n",
        "    answer_to_option = {\n",
        "        \"yes\": \"A) yes\",\n",
        "        \"no\": \"B) no\",\n",
        "        \"maybe\": \"C) maybe\"\n",
        "    }\n",
        "\n",
        "    correct_answer = example['final_decision']\n",
        "    correct_answer = answer_to_option.get(correct_answer, \"Unknown\")\n",
        "\n",
        "    correct_answer = f\"Answer: {correct_answer}\"\n",
        "\n",
        "    options_str = \"\\n\".join(options)\n",
        "    input_text = f\"Context: {context}\\nQuestion: {question}\\nOptions:\\n{options_str}\"\n",
        "\n",
        "    # Create a ChatML-like conversation for each data point\n",
        "    conversation = [\n",
        "        {\"from\": \"human\", \"value\": input_text},\n",
        "        {\"from\": \"gpt\", \"value\": correct_answer}\n",
        "    ]\n",
        "\n",
        "    # Add the conversation to the list\n",
        "    converted_conversations.append(conversation)\n",
        "\n",
        "formatted_pub_medqa_dataset = Dataset.from_dict({\"conversations\": converted_conversations})\n",
        "\n",
        "formatted_pub_medqa_dataset = formatted_pub_medqa_dataset.map(apply_template, batched=True)"
      ],
      "metadata": {
        "id": "aQlmo35tYzBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(\"MedQA dataset:\", formatted_medqa_dataset)\n",
        "#print(\"MedMCQA dataset:\", formatted_med_mcqa_dataset)\n",
        "#print(\"PubMedQA dataset:\", formatted_pub_medqa_dataset)\n",
        "\n",
        "#print(formatted_medqa_dataset[1])\n",
        "#print(formatted_med_mcqa_dataset[15])\n",
        "#print(formatted_pub_medqa_dataset[0])\n",
        "print(formatted_medqa_dataset[0][\"text\"])\n",
        "print(formatted_med_mcqa_dataset[0][\"text\"])\n",
        "print(formatted_pub_medqa_dataset[9][\"text\"])"
      ],
      "metadata": {
        "id": "L5jJE1SxIeeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#concatenate  datasets\n",
        "\n",
        "combined_dataset = concatenate_datasets([formatted_medqa_dataset, formatted_med_mcqa_dataset, formatted_pub_medqa_dataset])\n",
        "combined_dataset = combined_dataset.shuffle(seed=42)\n",
        "\n",
        "print(\"combined_dataset:\", combined_dataset)\n",
        "\n",
        "train_val_split = combined_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = train_val_split['train']\n",
        "val_dataset = train_val_split['test']\n",
        "\n",
        "print(\"training dataset:\", train_dataset)\n",
        "print(\"validation dataset:\", val_dataset)"
      ],
      "metadata": {
        "id": "e-se_WXjgwxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcPAQihcjcfl"
      },
      "outputs": [],
      "source": [
        "trainer=SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=True,\n",
        "    args=TrainingArguments(\n",
        "        learning_rate=3e-4,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        num_train_epochs=1,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=360,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        warmup_steps=10,\n",
        "        output_dir=\"output\",\n",
        "        seed=0,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=360\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "#Below is how to resume training if you are interrupted in the middle of the fine-tuning process.\n",
        "#trainer.train(resume_from_checkpoint=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORa-rPvGmT9p"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained_merged(\"model_dir\", tokenizer, save_method=\"merged_16bit\")\n",
        "#model.push_to_hub_merged(\"mlabonne/FineLlama-3.1-8B\", tokenizer, save_method=\"merged_16bit\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"/content/gdrive/My Drive/TRLWS/model_dir\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")"
      ],
      "metadata": {
        "id": "gulBo4FExBqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference on MedQA test set\n",
        "# Load model for inference\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\", \"system\": \"system\"}, # ShareGPT style\n",
        "    map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Load test dataset\n",
        "test_dataset = load_dataset(\"GBaker/MedQA-USMLE-4-options-hf\", split=\"test\")\n",
        "#print(test_dataset)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for example in tqdm(test_dataset):\n",
        "  question = example['sent1']\n",
        "\n",
        "  options = [\n",
        "      f\"A) {example['ending0']}\",\n",
        "      f\"B) {example['ending1']}\",\n",
        "      f\"C) {example['ending2']}\",\n",
        "      f\"D) {example['ending3']}\"\n",
        "  ]\n",
        "  correct_answer = chr(65 + example['label'])  # Convert 0-3 to A-D\n",
        "  print(\"Correct Answer:\", correct_answer)\n",
        "\n",
        "  # Prepare input\n",
        "  options_str = \"\\n\".join(options)\n",
        "  input_text = f\"Question: {question}\\nOptions:\\n{options_str}\\nAnswer:\"\n",
        "  #print(input_text)\n",
        "  system_message = \"You are a highly knowledgeable medical assistant with expertise in various medical domains. Your primary role is to provide accurate, evidence-based medical information and advice. Your responses should be grounded in the latest clinical guidelines and research, ensuring reliability and precision. When answering questions, consider the context and complexity of the query, and aim to provide the most relevant and correct answers.\"\n",
        "  messages = [\n",
        "      {\"from\": \"system\", \"value\": system_message},\n",
        "      {\"from\": \"human\", \"value\": input_text}\n",
        "  ]\n",
        "\n",
        "  inputs = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      tokenize = True,\n",
        "      add_generation_prompt = True, # Must add for generation\n",
        "      return_tensors = \"pt\",\n",
        "  ).to(\"cuda\")\n",
        "\n",
        "\n",
        "  outputs = model.generate(input_ids = inputs, max_new_tokens = 128)\n",
        "  predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "  #print(\"predicted text:\",predicted_text)\n",
        "\n",
        "  predicted_answer = \"\"\n",
        "\n",
        "  # Define the possible options\n",
        "  options_list = [\"A\", \"B\", \"C\", \"D\"]\n",
        "\n",
        "  # Search for the pattern \"Answer: <option>\"\n",
        "  for option in options_list:\n",
        "      if f\"Answer: {option}\" in predicted_text:\n",
        "          predicted_answer += option\n",
        "  print(\"Predicted Answer:\",predicted_answer)\n",
        "\n",
        "  # Check accuracy\n",
        "  if predicted_answer == correct_answer:\n",
        "    correct += 1\n",
        "    print(\"Accurately predicted\")\n",
        "  else:\n",
        "    print(\"Prediction is wrong\")\n",
        "\n",
        "  total += 1\n",
        "\n",
        "\n",
        "accuracy = correct / total\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x3IKcx1cr9xE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference on MedMCQA validation set\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\", \"system\": \"system\"}, # ShareGPT style\n",
        "    map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
        ")\n",
        "\n",
        "# Load model for inference\n",
        "model = FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Load test dataset\n",
        "test_dataset = load_dataset(\"openlifescienceai/medmcqa\", split=\"validation\")\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for example in tqdm(test_dataset):\n",
        "  #\n",
        "  question = example['question']\n",
        "\n",
        "  options = [\n",
        "      f\"A) {example['opa']}\",\n",
        "      f\"B) {example['opb']}\",\n",
        "      f\"C) {example['opc']}\",\n",
        "      f\"D) {example['opd']}\"\n",
        "  ]\n",
        "\n",
        "  correct_answer = chr(65 + example['cop'])  # Convert 0-3 to A-D\n",
        "  print(\"Correct Answer:\", correct_answer)\n",
        "\n",
        "  # Prepare input\n",
        "  options_str = \"\\n\".join(options)\n",
        "  input_text = f\"Question: {question}\\nOptions:\\n{options_str}\\nAnswer:\"\n",
        "\n",
        "\n",
        "  system_message = \"You are a highly knowledgeable medical assistant with expertise in various medical domains. Your primary role is to provide accurate, evidence-based medical information and advice. Your responses should be grounded in the latest clinical guidelines and research, ensuring reliability and precision. When answering questions, consider the context and complexity of the query, and aim to provide the most relevant and correct answers.\"\n",
        "  messages = [\n",
        "      {\"from\": \"system\", \"value\": system_message},\n",
        "      {\"from\": \"human\", \"value\": input_text}\n",
        "  ]\n",
        "\n",
        "  inputs = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      tokenize = True,\n",
        "      add_generation_prompt = True, # Must add for generation\n",
        "      return_tensors = \"pt\",\n",
        "  ).to(\"cuda\")\n",
        "\n",
        "\n",
        "  outputs = model.generate(input_ids = inputs, max_new_tokens = 128)\n",
        "  predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "  #print(\"predicted text:\",predicted_text)\n",
        "  predicted_answer = \"\"\n",
        "\n",
        "  # Define the possible options\n",
        "  options_list = [\"A\", \"B\", \"C\", \"D\"]\n",
        "\n",
        "  # Search for the pattern \"Answer: <option>\"\n",
        "  for option in options_list:\n",
        "      if f\"Answer: {option}\" in predicted_text:\n",
        "          predicted_answer += option\n",
        "  print(\"Predicted Answer:\", predicted_answer)\n",
        "\n",
        "  # Check accuracy\n",
        "  if predicted_answer == correct_answer:\n",
        "    correct += 1\n",
        "    print(\"Acurately predicted\")\n",
        "  else:\n",
        "    print(\"Prediction is wrong\")\n",
        "\n",
        "  total += 1\n",
        "\n",
        "\n",
        "accuracy = correct / total\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n"
      ],
      "metadata": {
        "id": "WvdnharBi0Sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference on PubMedQA validation set\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\", \"system\": \"system\"}, # ShareGPT style\n",
        "    map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
        ")\n",
        "\n",
        "# Load model for inference\n",
        "model = FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Load test dataset\n",
        "test_dataset = load_dataset(\"qiaojin/PubMedQA\", \"pqa_labeled\", split=\"train\")\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for example in tqdm(test_dataset):\n",
        "  #\n",
        "  context = example['context']\n",
        "  context = \" \".join(context['contexts'])\n",
        "  question = example['question']\n",
        "\n",
        "  options = [\n",
        "      \"A) yes\",\n",
        "      \"B) no\",\n",
        "      \"C) maybe\"\n",
        "  ]\n",
        "\n",
        "  # Create a mapping from answer to option\n",
        "  answer_to_option = {\n",
        "      \"yes\": \"A\",\n",
        "      \"no\": \"B\",\n",
        "      \"maybe\": \"C\"\n",
        "  }\n",
        "\n",
        "  correct_answer = example['final_decision']\n",
        "  correct_answer = answer_to_option.get(correct_answer, \"Unknown\")\n",
        "  #correct_answer = chr(65 + example['final_decision'])  # Convert 0-3 to A-D\n",
        "  print(\"Correct Answer:\", correct_answer)\n",
        "\n",
        "  # Prepare input\n",
        "  options_str = \"\\n\".join(options)\n",
        "  input_text = f\"Context: {context}\\nQuestion: {question}\\nOptions:\\n{options_str}\\nAnswer:\"\n",
        "\n",
        "  system_message = \"You are a highly knowledgeable medical assistant with expertise in various medical domains. Your primary role is to provide accurate, evidence-based medical information and advice. Your responses should be grounded in the latest clinical guidelines and research, ensuring reliability and precision. When answering questions, consider the context and complexity of the query, and aim to provide the most relevant and correct answers.\"\n",
        "  messages = [\n",
        "      {\"from\": \"system\", \"value\": system_message},\n",
        "      {\"from\": \"human\", \"value\": input_text}\n",
        "  ]\n",
        "\n",
        "  inputs = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      tokenize = True,\n",
        "      add_generation_prompt = True, # Must add for generation\n",
        "      return_tensors = \"pt\",\n",
        "  ).to(\"cuda\")\n",
        "\n",
        "\n",
        "  outputs = model.generate(input_ids = inputs, max_new_tokens = 128)\n",
        "  predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "  #print(\"predicted text:\",predicted_text)\n",
        "  predicted_answer = \"\"\n",
        "\n",
        "  # Define the possible options\n",
        "  options_list = [\"A\", \"B\", \"C\"]\n",
        "\n",
        "  # Search for the pattern \"Answer: <option>\"\n",
        "  for option in options_list:\n",
        "      if f\"Answer: {option}\" in predicted_text:\n",
        "          predicted_answer += option\n",
        "  print(\"Predicted Answer:\", predicted_answer)\n",
        "\n",
        "  # Check accuracy\n",
        "  if predicted_answer == correct_answer:\n",
        "    correct += 1\n",
        "    print(\"Acurately predicted\")\n",
        "  else:\n",
        "    print(\"Prediction is wrong\")\n",
        "\n",
        "  total += 1\n",
        "\n",
        "\n",
        "accuracy = correct / total\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n"
      ],
      "metadata": {
        "id": "xc5L3-SzOELv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
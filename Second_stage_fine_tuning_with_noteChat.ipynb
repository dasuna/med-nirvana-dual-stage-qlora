{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq \"xformers<0.0.24\""
      ],
      "metadata": {
        "id": "0rkwSED8E1bY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ],
      "metadata": {
        "id": "4OF4HxtME8gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq --no-deps \"trl<0.9.0\" peft accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "2KMq_J40FA2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJfancSS3a6t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from datasets import Dataset, load_dataset, concatenate_datasets\n",
        "from transformers import TrainingArguments, TextStreamer\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "max_seq_length = 2048\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"Dasun7/Med-mix-Llama-3.1-8B\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=True,\n",
        "    dtype=None,\n",
        ")\n",
        "\n",
        "# Prepare model for PEFT\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n",
        "    use_rslora=True,\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state = 0\n",
        ")\n",
        "print(model.print_trainable_parameters())"
      ],
      "metadata": {
        "id": "y0aYhZLDIRdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\", \"system\": \"system\"}, # ShareGPT style\n",
        "    map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
        ")\n",
        "\n",
        "def apply_template(examples):\n",
        "    # Prepend the system message\n",
        "    system_message = [{\"from\": \"system\", \"value\": \"You are a highly knowledgeable medical assistant with expertise in various medical domains. Your primary role is to provide accurate, evidence-based medical information and advice. Your responses should be grounded in the latest clinical guidelines and research, ensuring reliability and precision. When answering questions, consider the context and complexity of the query, and aim to provide the most relevant and correct answers.\"}]\n",
        "    formatted_conversations = []\n",
        "    for example in examples[\"conversations\"]:\n",
        "        # Combine the system message with the user/assistant conversation\n",
        "        conversation = system_message + example\n",
        "        formatted_conversations.append(conversation)\n",
        "\n",
        "    text = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in formatted_conversations]\n",
        "    return {\"text\": text}\n",
        "\n"
      ],
      "metadata": {
        "id": "bCNVTSicIe06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "note_chat_dataset = load_dataset(\"akemiH/NoteChat\", split=\"train\")"
      ],
      "metadata": {
        "id": "WlSIxh-k35HZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the Hugging Face datasets to Pandas DataFrames\n",
        "df2 = pd.DataFrame(note_chat_dataset)"
      ],
      "metadata": {
        "id": "pVHodbwL44fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert conversation format in the second dataset to match the first dataset's format\n",
        "def convert_conversation(conversation):\n",
        "    conversation_list = []\n",
        "\n",
        "    for line in conversation.splitlines():\n",
        "        line = line.strip()\n",
        "        if not line or line.startswith(\"[\"):  # Skip empty lines and descriptive text in brackets\n",
        "            continue\n",
        "\n",
        "        if line.startswith(\"Doctor:\"):\n",
        "            conversation_list.append({\n",
        "                \"from\": \"gpt\",\n",
        "                \"value\": line.replace(\"Doctor:\", \"\").strip()\n",
        "            })\n",
        "        else:\n",
        "            # Assume any line not starting with \"Doctor:\" is from the human\n",
        "            if \": \" in line:\n",
        "                speaker, text = line.split(\": \", 1)\n",
        "                conversation_list.append({\n",
        "                    \"from\": \"human\",\n",
        "                    \"value\": text.strip()\n",
        "                })\n",
        "            else:\n",
        "                conversation_list.append({\n",
        "                    \"from\": \"human\",\n",
        "                    \"value\": line.strip()\n",
        "                })\n",
        "\n",
        "    return conversation_list\n",
        "\n",
        "\n",
        "# Apply the conversion function to the \"conversation\" column in df2\n",
        "# Replace 'conversation' with the actual column name in df2\n",
        "df2[\"conversations\"] = df2[\"conversation\"].apply(convert_conversation)\n",
        "\n",
        "df2 = df2.drop(columns=[\"conversation\"])\n",
        "df2 = df2.drop(columns=[\"data\"])\n",
        "\n",
        "converted_notechat_dataset = Dataset.from_pandas(df2)\n",
        "\n",
        "\n",
        "print(converted_notechat_dataset)"
      ],
      "metadata": {
        "id": "KTeQPrs-7F5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "converted_notechat_dataset = converted_notechat_dataset.map(apply_template, batched=True)"
      ],
      "metadata": {
        "id": "E23qJ79xLoU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(converted_notechat_dataset)\n",
        "print(converted_notechat_dataset[92795][\"conversations\"])\n",
        "print(converted_notechat_dataset[92795][\"text\"])"
      ],
      "metadata": {
        "id": "yj2V3XwxTe5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the dataset with the specified seed\n",
        "shuffled_notechat_dataset = converted_notechat_dataset.shuffle(seed=42)\n",
        "\n",
        "#*************************\n",
        "# Select the first 100,000 instances for the training set\n",
        "notechat_train_dataset = shuffled_notechat_dataset.select(range(100000))\n",
        "\n",
        "#******************************\n",
        "# Select the next 10,000 instances for the validation set\n",
        "notechat_val_dataset = shuffled_notechat_dataset.select(range(100000, 110000))\n",
        "\n",
        "print(notechat_train_dataset)\n",
        "print(notechat_val_dataset)"
      ],
      "metadata": {
        "id": "9xPZzngaczZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# peparing MedQA dataset\n",
        "\n",
        "medqa_dataset = load_dataset(\"GBaker/MedQA-USMLE-4-options-hf\", split=\"train\")\n",
        "\n",
        "converted_conversations = []\n",
        "\n",
        "for example in medqa_dataset:\n",
        "    # Extract the question and answer options\n",
        "    question = example['sent1']\n",
        "    #options = [example['ending0'], example['ending1'], example['ending2'], example['ending3']]\n",
        "    options = [\n",
        "        f\"A) {example['ending0']}\",\n",
        "        f\"B) {example['ending1']}\",\n",
        "        f\"C) {example['ending2']}\",\n",
        "        f\"D) {example['ending3']}\"\n",
        "    ]\n",
        "\n",
        "\n",
        "    correct_answer = f\"Answer: {options[example['label']]}\"\n",
        "\n",
        "    options_str = \"\\n\".join(options)\n",
        "    input_text = f\"Question: {question}\\nOptions:\\n{options_str}\"\n",
        "\n",
        "    # Create a ChatML-like conversation for each data point\n",
        "    conversation = [\n",
        "        {\"from\": \"human\", \"value\": input_text},\n",
        "        {\"from\": \"gpt\", \"value\": correct_answer}\n",
        "    ]\n",
        "\n",
        "    # Add the conversation to the list\n",
        "    converted_conversations.append(conversation)\n",
        "\n",
        "formatted_medqa_dataset = Dataset.from_dict({\"conversations\": converted_conversations})\n",
        "\n",
        "formatted_medqa_dataset = formatted_medqa_dataset.map(apply_template, batched=True)\n"
      ],
      "metadata": {
        "id": "4KCyj2vMdqy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# peparing MedMCQA dataset\n",
        "\n",
        "med_mcqa_dataset = load_dataset(\"openlifescienceai/medmcqa\", split=\"train\")\n",
        "\n",
        "converted_conversations = []\n",
        "\n",
        "for example in med_mcqa_dataset:\n",
        "    # Extract the question and answer options\n",
        "    question = example['question']\n",
        "    #options = [example['ending0'], example['ending1'], example['ending2'], example['ending3']]\n",
        "    options = [\n",
        "        f\"A) {example['opa']}\",\n",
        "        f\"B) {example['opb']}\",\n",
        "        f\"C) {example['opc']}\",\n",
        "        f\"D) {example['opd']}\"\n",
        "    ]\n",
        "\n",
        "\n",
        "    correct_answer = f\"Answer: {options[example['cop']]}\"\n",
        "\n",
        "    options_str = \"\\n\".join(options)\n",
        "    input_text = f\"Question: {question}\\nOptions:\\n{options_str}\"\n",
        "\n",
        "    # Create a ChatML-like conversation for each data point\n",
        "    conversation = [\n",
        "        {\"from\": \"human\", \"value\": input_text},\n",
        "        {\"from\": \"gpt\", \"value\": correct_answer}\n",
        "    ]\n",
        "\n",
        "    # Add the conversation to the list\n",
        "    converted_conversations.append(conversation)\n",
        "\n",
        "formatted_med_mcqa_dataset = Dataset.from_dict({\"conversations\": converted_conversations})\n",
        "\n",
        "#print(formatted_med_mcqa_dataset[0])\n",
        "\n",
        "formatted_med_mcqa_dataset = formatted_med_mcqa_dataset.map(apply_template, batched=True)"
      ],
      "metadata": {
        "id": "SktcpAf3edJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# peparing PubMedQA dataset\n",
        "\n",
        "pub_medqa_dataset = load_dataset(\"qiaojin/PubMedQA\", \"pqa_artificial\", split=\"train\")\n",
        "\n",
        "converted_conversations = []\n",
        "\n",
        "for example in pub_medqa_dataset:\n",
        "    # Extract the question and answer options\n",
        "    context = example['context']\n",
        "\n",
        "    context = \" \".join(context['contexts'])\n",
        "    question = example['question']\n",
        "\n",
        "    options = [\n",
        "        f\"A) yes\",\n",
        "        f\"B) no\",\n",
        "        f\"C) maybe\"\n",
        "    ]\n",
        "\n",
        "    answer_to_option = {\n",
        "        \"yes\": \"A) yes\",\n",
        "        \"no\": \"B) no\",\n",
        "        \"maybe\": \"C) maybe\"\n",
        "    }\n",
        "\n",
        "    correct_answer = example['final_decision']\n",
        "    correct_answer = answer_to_option.get(correct_answer, \"Unknown\")\n",
        "\n",
        "    correct_answer = f\"Answer: {correct_answer}\"\n",
        "\n",
        "    options_str = \"\\n\".join(options)\n",
        "    input_text = f\"Context: {context}\\nQuestion: {question}\\nOptions:\\n{options_str}\"\n",
        "\n",
        "    # Create a ChatML-like conversation for each data point\n",
        "    conversation = [\n",
        "        {\"from\": \"human\", \"value\": input_text},\n",
        "        {\"from\": \"gpt\", \"value\": correct_answer}\n",
        "    ]\n",
        "\n",
        "    # Add the conversation to the list\n",
        "    converted_conversations.append(conversation)\n",
        "\n",
        "formatted_pub_medqa_dataset = Dataset.from_dict({\"conversations\": converted_conversations})\n",
        "\n",
        "formatted_pub_medqa_dataset = formatted_pub_medqa_dataset.map(apply_template, batched=True)"
      ],
      "metadata": {
        "id": "nH8WWSFQegSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#concatenate  datasets\n",
        "\n",
        "combined_dataset = concatenate_datasets([formatted_medqa_dataset, formatted_med_mcqa_dataset, formatted_pub_medqa_dataset])\n",
        "combined_dataset = combined_dataset.shuffle(seed=42)\n",
        "\n",
        "print(\"combined_dataset:\", combined_dataset)\n",
        "\n",
        "train_val_split = combined_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "medmix_complete_train_dataset = train_val_split['train']\n",
        "\n",
        "# Shuffle the dataset with the specified seed\n",
        "shuffled_medmix_complete_train_dataset = medmix_complete_train_dataset.shuffle(seed=42)\n",
        "\n",
        "#*************************\n",
        "# Select the first 33,333 instances for the training set\n",
        "medmix_train_dataset = shuffled_medmix_complete_train_dataset.select(range(33333))\n",
        "\n",
        "\n",
        "medmix_complete_val_dataset = train_val_split['test']\n",
        "\n",
        "# Shuffle the dataset with the specified seed\n",
        "shuffled_medmix_complete_val_dataset = medmix_complete_val_dataset.shuffle(seed=42)\n",
        "\n",
        "#*************************\n",
        "# Select the first 33,33 instances for the validation set\n",
        "medmix_val_dataset = shuffled_medmix_complete_val_dataset.select(range(3333))\n",
        "\n",
        "print(medmix_train_dataset)\n",
        "print(medmix_val_dataset)\n"
      ],
      "metadata": {
        "id": "YK1yQk76ejUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_train_dataset = concatenate_datasets([notechat_train_dataset, medmix_train_dataset])\n",
        "final_train_dataset = final_train_dataset.shuffle(seed=42)\n",
        "\n",
        "print(final_train_dataset)"
      ],
      "metadata": {
        "id": "9H22PhGUhbQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_val_dataset = concatenate_datasets([notechat_val_dataset, medmix_val_dataset])\n",
        "final_val_dataset = final_val_dataset.shuffle(seed=42)\n",
        "\n",
        "print(final_val_dataset)"
      ],
      "metadata": {
        "id": "WTkhPfvQh_wQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer=SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=final_train_dataset,\n",
        "    eval_dataset=final_val_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=True,\n",
        "    args=TrainingArguments(\n",
        "        learning_rate=3e-4,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        num_train_epochs=1,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=360,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        warmup_steps=10,\n",
        "        output_dir=\"output\",\n",
        "        seed=0,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=360\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "#Below is how to resume training if you are interrupted in the middle of the fine-tuning process.\n",
        "#trainer.train(resume_from_checkpoint=True)"
      ],
      "metadata": {
        "id": "CHQCwHWMkIF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub_merged(\"Dasun7/Med-nirvana-v1-8B\", tokenizer, save_method=\"merged_16bit\")"
      ],
      "metadata": {
        "id": "Pps3u2o5kfPd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}